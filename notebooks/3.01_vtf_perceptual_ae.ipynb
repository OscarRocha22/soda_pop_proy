{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9099349",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# <font color= #f6c049> **Soda Pop Project: Data Processing** </font>\n",
    "#### <font color= #2E9AFE> `Deep Learning`</font>\n",
    "<Strong> Sofía Maldonado, Óscar Josué Rocha & Viviana Toledo </Strong>\n",
    "\n",
    "_27/02/2026._\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f015069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Models\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Concatenate, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras import Model\n",
    "\n",
    "# Loss Function\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras import Model, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "678f665c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8487dd95",
   "metadata": {},
   "source": [
    "# <font color= #f6c049> **Modeling** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b855103",
   "metadata": {},
   "source": [
    "The selected model architecture for the image generation task will be a **Convolutional Autoencoder with a Perceptual loss function using VGG-19**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19c8f5b",
   "metadata": {},
   "source": [
    "## <font color= #dba226> &ensp; • **Perceptual Loss with VGG Model** </font>\n",
    "\n",
    "The final convolutional layer from the VGG19 model is going to be extracted to use as a loss function. This layer contains deep patterns captured during training. The weights of the model will be freezed so that no more training is performed, and the layer is only used to compare the input images versus the generated ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6948e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vgg_model():\n",
    "    # Get VGG model trained with imagenet, with no fixed input shape\n",
    "    vgg = VGG19(weights='imagenet', include_top=False, input_shape=(None, None, 3))\n",
    "    vgg.trainable = False           # Freeze weights\n",
    "    loss_model = Model(inputs=vgg.input, outputs=vgg.get_layer('block5_conv4').output)\n",
    "    loss_model.trainable = False\n",
    "    return loss_model\n",
    "\n",
    "vgg_model = get_vgg_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e25a22",
   "metadata": {},
   "source": [
    "With the previously extracted layer, a customized loss function is defined by calculating the difference between predicted and true images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21185ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptual_loss(y_true, y_pred):\n",
    "    y_true_features = vgg_model(y_true)\n",
    "    y_pred_features = vgg_model(y_pred)\n",
    "    return tf.reduce_mean(tf.square(y_true_features - y_pred_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b696ec6",
   "metadata": {},
   "source": [
    "## <font color= #dba226> &ensp; • **Convolutional Autoencoder** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a31c7b",
   "metadata": {},
   "source": [
    "Before starting the CAE, we import the previously normalized and resized images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72cb8ef8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "tf.data snapshot element_spec file not found: ../data/processed/test_ds/dataset_spec.pb.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m train_ds = tf.data.Dataset.load(\u001b[33m'\u001b[39m\u001b[33m../data/processed/train_ds\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m val_ds = tf.data.Dataset.load(\u001b[33m'\u001b[39m\u001b[33m../data/processed/val_ds\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m test_ds = \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../data/processed/test_ds\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/apps/deep-learning/soda_pop_proy/.venv/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py:1846\u001b[39m, in \u001b[36mDatasetV2.load\u001b[39m\u001b[34m(path, element_spec, compression, reader_func, wait)\u001b[39m\n\u001b[32m   1842\u001b[39m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops -> load_op ->\u001b[39;00m\n\u001b[32m   1843\u001b[39m \u001b[38;5;66;03m# dataset_ops).\u001b[39;00m\n\u001b[32m   1844\u001b[39m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[32m   1845\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_op\n\u001b[32m-> \u001b[39m\u001b[32m1846\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_op\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1847\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[43m    \u001b[49m\u001b[43melement_spec\u001b[49m\u001b[43m=\u001b[49m\u001b[43melement_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1849\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1850\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreader_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreader_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1851\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/apps/deep-learning/soda_pop_proy/.venv/lib/python3.11/site-packages/tensorflow/python/data/ops/load_op.py:68\u001b[39m, in \u001b[36m_load\u001b[39m\u001b[34m(path, element_spec, compression, reader_func, wait)\u001b[39m\n\u001b[32m     64\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m _load_distributed_snapshot(\n\u001b[32m     65\u001b[39m       path, distributed_snapshot_metadata, reader_func)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m   element_spec = \u001b[43m_load_element_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _LoadDataset(path, element_spec, compression, reader_func)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/apps/deep-learning/soda_pop_proy/.venv/lib/python3.11/site-packages/tensorflow/python/data/ops/load_op.py:155\u001b[39m, in \u001b[36m_load_element_spec\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    153\u001b[39m dataset_spec_filename = os.path.join(path, dataset_ops.DATASET_SPEC_FILENAME)\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gfile.Exists(dataset_spec_filename):\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m errors.NotFoundError(\n\u001b[32m    156\u001b[39m       node_def=\u001b[38;5;28;01mNone\u001b[39;00m, op=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    157\u001b[39m       message=\u001b[33m\"\u001b[39m\u001b[33mtf.data snapshot element_spec file not found: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    158\u001b[39m               \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_spec_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m gfile.GFile(dataset_spec_filename, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    161\u001b[39m   encoded_spec = f.read()\n",
      "\u001b[31mNotFoundError\u001b[39m: tf.data snapshot element_spec file not found: ../data/processed/test_ds/dataset_spec.pb."
     ]
    }
   ],
   "source": [
    "train_ds = tf.data.Dataset.load('../data/processed/train_ds')\n",
    "val_ds = tf.data.Dataset.load('../data/processed/val_ds')\n",
    "test_ds = tf.data.Dataset.load('../data/processed/test_ds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98729464",
   "metadata": {},
   "source": [
    "The images have a size of 256x256"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soda-gen-proy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
